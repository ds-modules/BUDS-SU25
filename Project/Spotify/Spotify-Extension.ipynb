{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Extension: Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. <a href='#section 1'>Formulating a Question or Problem</a>\n",
    "\n",
    "\n",
    "2. <a href='#section 2'>Acquiring and Preparing Data</a>\n",
    "\n",
    "\n",
    "3. <a href='#section 3'>Conducting Exploratory Data Analysis</a>\n",
    "\n",
    "\n",
    "4. <a href='#section 4'>Using Prediction and Inference to Draw Conclusions</a>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Knowledge\n",
    "\n",
    "If you listen to music, chances are you use Spotify, Apple Music, or another similar streaming service. This new era of the music industry curates playlists, recommends new artists, and is based on the number of streams more than the number of albums sold. The way these streaming services do this is (you guessed it) data!\n",
    "\n",
    "Spotify, like many other companies, hires many full-time data scientists to analyze all the incoming user data and use it to make predictions and recommendations for users. If you're interested, feel free to check out [Spotify's Engineering Page](https://engineering.atspotify.com/) for more information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spotify.png\" width = 700/>\n",
    "\n",
    "<center><a href=https://hrblog.spotify.com/2018/02/08/amping-up-diversity-inclusion-at-spotify/>Image Reference</a></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Readings\n",
    "\n",
    "The following articles, which talk about identifying musical genres using machine learning and artificial intelligence, will help provide context to this notebook:\n",
    "1. [AI Detecting Genres](https://www.engadget.com/2017-05-04-machine-learning-ai-identifies-music-genres.html): AI can recognize musical genres better than humans, but it won’t shame you for liking that one Justin Bieber song.\n",
    "\n",
    "2. [Dubolt](https://dubolt.com/): A web application that uses Spotify audio features to recommend songs!\n",
    "\n",
    "\n",
    "After reading these articles, come up with five observations / reflections / questions and enter them below:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Science Life Cycle <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formulating a Question or Problem <a id='section 1'></a>\n",
    "It is important to ask questions that will be informative and can be answered using the data. There are many different questions we could ask about music data. For example, there are many artists who want to find out how to get their music on Spotify's Discover Weekly playlist in order to gain exposure. Similarly, users love to see their *Spotify Wrapped* listening reports at the end of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a <i>new</i> research question that still remains unanswered after your initial investigation. For those who did not complete the Spotify project, work with another intern who has.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Original Question(s):** *here*\n",
    "\n",
    "\n",
    "**Data you would need:** *here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Question:</b> Now, taking a look at the data you already have, think of a few questions that you <b>could</b>\n",
    "    answer with the current data set.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your questions:** *here*\n",
    "\n",
    "**Columns / information you would need:** *here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Acquiring and Preparing Data <a id='section 2'></a>\n",
    "\n",
    "We'll be looking at song data from Spotify. You can find the raw data [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-01-21). We've cleaned up the datasets a bit, and we will be investigating the popularity and the qualities of songs from this dataset.\n",
    "\n",
    "The following table, `spotify`, contains a list of tracks identified by their unique song ID along with attributes about that track.\n",
    "\n",
    "Here are the descriptions of the columns for your reference. We may not be using all of these fields, but you can still take a look at its description.\n",
    "\n",
    "|Variable Name   | Description |\n",
    "|--------------|------------|\n",
    "|`track_id` | \tSong unique ID |\n",
    "|`track_name` | Song name |\n",
    "|`track_artist\t`| Song artist |\n",
    "|`track_popularity` | Song popularity (0-100), where higher is better |\n",
    "|`track_album_id`| Album unique ID |\n",
    "|`track_album_name` | Song album name |\n",
    "|`track_album_release_date`| Date when album was released |\n",
    "|`playlist_name`| Name of playlist |\n",
    "|`playlist_id`| Playlist ID |\n",
    "|`playlist_genre`| Playlist genre |\n",
    "|`playlist_subgenre\t`|  Playlist subgenre |\n",
    "|`danceability`| Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |\n",
    "|`energy`| Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |\n",
    "|`key`| The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation (e.g., 0 = C, 1 = C♯/D♭, 2 = D, and so on). If no key was detected, the value is -1. |\n",
    "|`loudness`|  The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 dB and 0 dB. |\n",
    "|`mode`|  Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. |\n",
    "|`speechiness`|  Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g., talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |\n",
    "|`acousticness`|  A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. |\n",
    "|`instrumentalness`| Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |\n",
    "|`liveness`| Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |\n",
    "|`valence`| A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g., happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g., sad, depressed, angry). |\n",
    "|`tempo`| The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |\n",
    "|`duration_ms`| Duration of song in milliseconds |\n",
    "|`creation_year`| Year when album was released |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify = Table.read_table('data/spotify.csv')\n",
    "spotify.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since today's research focuses on analyzing differences between audio features, we can remove a few of the unnecessary columns above. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = make_array('track_id', 'track_name', 'playlist_genre', 'playlist_subgenre', 'danceability',\n",
    "                'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms')\n",
    "spotify = spotify.select(desired_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spotify.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conducting Exploratory Data Analysis <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy for us as humans to distinguish between distinct music genres. We listen to the music and — just like magic — are able to tell what genre a song will fall into. However, how does a computer do this?\n",
    "\n",
    "In this section, we will investigate the audio features of different musical genres based on the songs in specific playlists. At the end, we will be able to see how a computer can tell the difference between **Rock** and **EDM**. We will be looking at the following feature columns.\n",
    "\n",
    "|Variable Name   | Description |\n",
    "|--------------|------------|\n",
    "|`playlist_genre`| Playlist genre |\n",
    "|`playlist_subgenre\t`|  Playlist subgenre |\n",
    "|`danceability`| Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |\n",
    "|`energy`| Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |\n",
    "|`key`| The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation (e.g., 0 = C, 1 = C♯/D♭, 2 = D, and so on). If no key was detected, the value is -1. |\n",
    "|`loudness`|  The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 dB and 0 dB. |\n",
    "|`mode`|  Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. |\n",
    "|`speechiness`|  Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g., talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |\n",
    "|`acousticness`|  A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. |\n",
    "|`instrumentalness`| Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |\n",
    "|`liveness`| Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |\n",
    "|`valence`| A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g., happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g., sad, depressed, angry). |\n",
    "|`tempo`| The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |\n",
    "|`duration_ms`| Duration of song in milliseconds |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Create two new tables called <code>rock</code> and <code>edm</code> which only have rows for the corresponding genre. Use the original table's <code>playlist_genre</code> column to obtain the genre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Distributions \n",
    "\n",
    "Recall that you have used bar charts to visualize distributions of categorical data. These visualizations show how many individuals are in each category. Bar charts have a similar counterpart called **histograms**, which can be used to visualize distributions of quantitative data.\n",
    "\n",
    "Let's practice creating histograms before we dive into our analysis. Create a histogram of the overall table's `danceability` column. Be sure to use the argument `normed=False` to prevent weird values from popping up and make sure that you increase the number of bins to get a better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> How can we interpret the graph we just created in the cell above? Why do the numbers range from 0 to 1? What does the shape of the plot tell us?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumentalness\n",
    "\n",
    "A track's **instrumentalness** predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context, whereas rap or spoken word tracks are considered “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a histogram for the <code>instrumentalness</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rock\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the histograms between rock and EDM. What do the data points around 0.8 represent for the EDM plot? Why might that exist?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence\n",
    "\n",
    "A track's **valence** is defined as a measure from 0.0 to 1.0 describing the musical positiveness conveyed by that track. Tracks with high valence sound more positive (e.g., happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g., sad, depressed, angry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a bar chart for the <code>valence</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rock\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the bar charts between rock and EDM. How do the distributions differ? Why do more EDM songs have lower valence?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo\n",
    "\n",
    "A track's **tempo** is measured in terms of beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a bar chart for the <code>tempo</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rock\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the bar charts between rock and EDM. Specifically, look at the counts on the <i>y</i>-axis. Make some comments about the <i>spread</i> of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Duration\n",
    "\n",
    "A track's **song duration** is the length of the track, in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a bar chart for the <code>duration_ms</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rock\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> What conclusions can we make about the average song length for a rock song vs. an EDM song?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at some major differences between `instrumentalness`, `valence`, `tempo`, and `duration_ms` between rock songs and EDM songs, it is your turn to investigate! In the following cells, please work with your partner / group to decide on:\n",
    "- Two genres to investigate (not Rock and EDM)\n",
    "- Three audio features to look at for each genre\n",
    "\n",
    "After making your selections, carry out a similar process as above on your own. If you get stuck, or are unsure where to start, feel free to look at the above cells for some examples (i.e., rock and EDM). Alternatively, please don't hesitate to reach out to your facilitators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using Prediction and Inference to Draw Conclusions <a id='section 4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at the different ways that audio features can be used to distinguish between certain musical genres, it is time to answer our intial question: \n",
    ">*How can machines tell the difference between different genres of music?*\n",
    "\n",
    "Answer the following questions to wrap up you extended research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> After comparing audio features of rock and EDM songs, tell us something interesting about this data. What details were you able to uncover?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> What are the limitations of using audio features to automatically identify genres? Can you think of an examples where this may be a bad approach?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> What other data could be used to determine a song's genre? How else can we use technology to group songs into categories?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <h2 class=\"alert-heading\">Well done!</h2>\n",
    "    <p>In this report, you used Spotify audio feature data from their Web API to determine how a machine can identify music genres automatically and how we can use data to <b>see</b> the difference between a rock song and an EDM song.\n",
    "    <hr>\n",
    "    <p> Notebook created for Berkeley Unboxing Data Science\n",
    "    <p> Adapted from Project 2: Spotify by Will Furtado with the support of Ani Adhikari, Deb Nolan, Carlos Ortiz, and Andrew Chen\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
